\chapter{Sorting with Divide-and-Conquer}

In this chapter we will explore some basic \emph{sorting} algorithms, all of them employing the \emph{Divide-and-Conquer} strategy for problem solving.

The fundamental idea of \emph{Divide-and-Conquer} algorithms is to divide a given problem into one or more subproblems, each of them beeing smaller than the original problem, to solve each of these subproblems, and then combine the solutions to give a solution to the original problem.

Each subproblem is simply the original problem on an input of smaller size; hence all subproblems are solved by the same strategy. A \emph{Divide-and-Conquer} algorithm is therefore essentially recursive in nature.

The subproblems are independent of each other (i.e. they do not share the same input data); hence they can be solved concurrently as well as sequentially. If the subproblems are not independent of each other, they could be tackled by the \emph{Dynamic Programming} strategy, which we will cover later in this text.

\emph{Divide-and-Conquer} algoritms are typically implemented within two separate procedures:
\begin{itemize}
\item \textbf{main}: the callable procedure, dividing up the work recursively and collecting the results from the worker method;
\item \textbf{worker}: repeatedly invoked procedure, computing the result for a subset of the original problem.
\end{itemize}

Before going into details of actual implementations, let us first define some desired qualities of sorting algorithms and algorithms in general.
Algorithms should be
\begin{itemize}
\item \textbf{fast}: this does not only concern the asymptotic runtime, which is relevant for big input sizes, but also means reducing the number of steps for each iteration of the algorithm; 
\item \textbf{smooth}: the more sorted the input is, the faster the algorithm should perform; in general, if the problem at hand is already solved for the majority of inputs values, the algorithm should be faster;
\item \textbf{stable}: sorting should not affect the presorted order of equal elements, i.e. the order of elements of a complex datatype with the same key should not be altered;
\item \textbf{compact}: the algorithm should be economical in its use of space as well as running time
\end{itemize}

We will see that a combination of these qualities for a distinct algorithm is often not easy to accomplish; especially the \emph{compactness} of an algorithm is difficult, if not impossible to achieve in a pure functional setting with \emph{immutable data}.

\section{MergeSort}

\emph{MergeSort} is a stable and fast sorting algorithm with a guaranteed runtime of $\Theta (n \lg n)$.
Compared to linear sorting algorithms like \emph{SelectionSort} and \emph{InsertionSort}, which share runtime of $\Theta (n^2)$ this means a huge performance advantage, as $\lg n$ grows much slower than $n$ itself for increasing input sizes.  
The downside of \emph{MergeSort} is the need of extra space (of size $n$), as no \emph{in-place-update} of the input structure is possible. So it is not compact. Likewise, \emph{MergeSort} isn't smooth, meaning that the algorithm will always take $\Theta (n \lg n)$ steps, even if the input is already sorted.

\begin{impl}[MergeSort with Haskell] \label{impl:mshaskell}
This implementation uses recursion for the main and worker method, as this is the most natural (and in a pure functional language the only) way to iterate through a data structure.
Observe, that there is no need of creating an explicit data structure for merging, as data structures are always build from scratch in a functional language with immutable data.
Also note the helper function \texttt{halve}, which divides an input list into to halves without traversing the input multiple times.
\end{impl}

\begin{haskellcode}
mSort :: Ord a => [a] -> [a]
mSort [] = []
mSort [x] = [x]
mSort xs = merge (mSort left) (mSort right)
           where (left, right) = halve xs

halve :: [a] -> ([a], [a])
halve = foldr op ([], []) where op x (ys, zs) = (zs, x:ys)

merge :: Ord a => [a] -> [a] -> [a]
merge [] ys = ys
merge xs [] = xs
merge left@(x:xs) right@(y:ys)
    | x <= y    = x : merge xs right
    | otherwise = y : merge left ys
\end{haskellcode}

\begin{impl}[MergeSort with Java] 
This implementation uses an auxiliary array for merging, which is filled in bitonic order:
the two subarrays for each recursion step are copied `back-to-back' into this array, such that the left subarray has ascending order, the right one descending order\footnote{as suggested in \autocite[chapter 8]{algsc98}}.
The actual sorting is done by comparing the first with the last element and copying the smaller one back into the original data structure. This implementation is fast and stable.
\end{impl}

\begin{javacode}
void mSort(int[] a, int l, int r) {
    int m = (r+l) / 2; // dividing input in two halves
    if (l >= r) return; // already sorted
    mSort(a, l, m);
    mSort(a, m+1, r);
    merge(a, l, m, r); // sort by merging
}

void merge(int[] a, int l, int m, int r) {
    int i, j;
    // copy the left half to aux
    for (i = m+1; i > l; i--) aux[i-1] = a[i-1];
    // copy the right half in reverse order to aux
    for (j = m; j < r; j++) aux[r+m-j] = a[j+1];
    // compare first and last value of aux and copy smaller to a
    for (int k = l; k <= r; k++) {
        if (aux[i] < aux[j]) a[k] = aux[i++];
        else a[k] = aux[j--];
    }
}
\end{javacode}

Observe that in both implementations the intermediate data structures are exclusively accessed in a sequential style, i.e. there is no need of random acces to an element at any stage of the process.
This, and the fact that the mandatory extra space is created anyway in a pure functional language makes \emph{MergeSort} a good choise for sorting in a functional setting with immutable data.

\subsection{A Functional Excursion on MergeSort}
It is wortwhile to explore some other, more sophisticated versions of \emph{MergeSort}, which will combine two or more of the desired qualities of algorithms.

Our basic implementation \ref{impl:mshaskell} (\nameref{impl:mshaskell}) is fast but not stable, due to the definition of the helper function \texttt{halve}.
This definition makes the algorithm unstable, as the order of the input elements is altered before sorting.
We could replace this definition with {\mintinline{haskell}{halve xs = (take m xs, drop m xs) where m = length xs div 2}}, and this would make the algorithm stable but also slower at the same time, as this definition has to iterate three times over the input list.

In particular, we will explore \emph{Bottom-up MergeSort}, which switches from the \emph{\textbf{divide-and-conquer}} strategy to a \emph{\textbf{bottom-up}} strategy, providing a fast and stable implementation.
Here we also have a \emph{main} and a \emph{worker} function, but now \emph{main} doesn't divide up the work recursively; instead it's merely a wrapper-function for calling the \emph{worker} with the correct arguments.
All the heavy work is done in the \emph{worker} function, which processes the whole input set at once, iterating over the input until the sorted list is produced.

Finally, we will take a look at \emph{Smooth MergeSort}, a variant of bottom-up with multiple runs, which (as its name suggests) is smooth as well\footnote{both examples are taken from \autocite[section 5.2]{adwh20}}.

\begin{impl}[Bottom-up MergeSort with Haskell]
The main function is \texttt{bumSort}, which calls the worker function \texttt{pairWith} with the correct parameters, thereby applying the helper functions \texttt{wrap, unwrap} and \texttt{single} for wrapping and unwrapping elements from the input.
Note that \texttt{bumSort} also uses the \texttt{merge} worker function from the standard \texttt{MergeSort} for merging the created pairs of lists.
\end{impl}

\begin{haskellcode}
bumSort :: Ord a => [a] -> [a]
bumSort [] = []
bumSort xs = unwrap (until single (pairWith merge) (map wrap xs))

pairWith f [] = []
pairWith f [x] = [x]
pairWith f (x:y:xs) = f x y:pairWith f xs

wrap x = [x]
unwrap [x] = x

single [x]  = True
single _    = False
\end{haskellcode}

Instead of giving a mathematical reasoning for \texttt{pairWith}, we will just demonstrate how it works on a given input of \texttt{[5,4,8,1,9,3,0,2,7,6]}:
\begin{verbatim}
pairWith merge (map wrap [5,4,8,1,9,3,0,2,7,6])
    [[4,5],[1,8],[3,9],[0,2],[6,7]]
pairWith merge [[4,5],[1,8],[3,9],[0,2],[6,7]]
    [[1,4,5,8],[0,2,3,9],[6,7]]
pairWith merge [[1,4,5,8],[0,2,3,9],[6,7]]
    [[0,1,2,3,4,5,8,9],[6,7]]
pairWith merge [[0,1,2,3,4,5,8,9],[6,7]]
    [[0,1,2,3,4,5,6,7,8,9]]
unwrap [[0,1,2,3,4,5,6,7,8,9]]
    [0,1,2,3,4,5,6,7,8,9]
\end{verbatim}

\begin{impl}[Smooth MergeSort with Haskell]
Instead of starting with a list of singleton lists (as \texttt{bumSort} does), we split the input into nondecreasing runs and begin the merging process with that.
The function \texttt{runs} processes the input from right to left.
Every next element is added to the front of the current run if it is samller or equal to the current element; otherwise it begins a new run.
\end{impl}

\begin{haskellcode}
smoothMS :: Ord a => [a] -> [a]
smoothMS [] = []
smoothMS xs = unwrap (until single (pairWith merge) (runs xs))

runs :: Ord a => [a] -> [[a]]
runs = foldr op []
    where op x [] = [[x]]
          op x ((y:xs):xss) | x <= y = (x:y:xs):xss
                            | otherwise = [x]:(y:xs):xss
\end{haskellcode}

Instead of starting a new run for every pair of lists, the \texttt{runs} function only starts a new run if an element is greater than its successor.
Therefore this implementation is not only fast and stable, but also smooth. In particular, if the input is already sorted, it needs only $\Theta(n)$ comparisons to return the input untouched.

\section{QickSort}

\emph{QuickSort} is a \emph{Divide-and-Conquer} algorithm as well, althoug it is sometimes referred to as a \emph{partitioning} or \emph{randomized} algorithm.
\emph{QuickSort} is stable, fast and compact; there is no need for an intermediate data structure because \emph{in-place-updates} are possible.

The main downside is that it is not smooth; on the contrary, the running will be $\Theta (n^2)$ for an already sorted input and extra care has to be taken in order to avoid this quadratic running time.

This behaviour is due to the choice of the pivot value, which is used to split the input list into two sublists, one with values smaller than the pivot, the other with values greater or equal than the pivot.

If the input is already sorted and we take the pivot always from the first position of the input, the list is split into an empty sublist and another with length $(n-1)$ for every turn, accumulating to $n$ partitioning steps for the whole algorithm.
The average running time is $\Theta (n \lg n)$, where the upper bound is $\mathcal{O}(n^2)$.

\begin{impl}[QuickSort with Haskell] \label{impl:qshaskell}
This implementation utilizes a partitioning function for splitting the input into two sublists. This reduces the additional needed space to $\Theta(n)$ and also keeps the algorithm fast, as this function has to iterate only once over the input for every turn.
Observe that no care is taken to choose a good pivot value, as this would lead to extra complexity in a functional setting, not improving the average runtime. As a result, this implementation is stable and fast on average, but not smooth.
\end{impl}

\begin{haskellcode}
qSort :: Ord a => [a] -> [a]
qSort [] = []
qSort (x:xs) = qSort ys ++ [x] ++ qSort zs
    where (ys, zs) = partition (<x) xs

partition :: (a -> Bool) -> [a] -> ([a], [a])
partition p = foldr op ([], [])
    where op x (ys, zs) = if p x then (x:ys, zs) else (ys, x:zs)
\end{haskellcode}

\begin{impl}[QuickSort with Java]
In order to avoid a quadratic runtime for already sorted inputs, the main method \texttt{qSort} calls a helper method \texttt{choosePivot} to randomly pick a value from within the given range, and puts this value in the first position.
The sorting is done in the worker method \texttt{partition}, where the pivot value is always taken from the first element (the randomly choosen element).
In each partitioning step the current pivot value is copied into its final position by shifting every smaller value to the left. With the random pivot value this implementation is fast, stable and compact, but not smooth.
\end{impl}

\begin{javacode}
void qSort(int[] a, int l, int r) {
    if (l >= r) return; // already sorted
    int i = choosePivot(l, r);
    swap(a, l, i); // make chosen pivot the first element
    int p = partition(a, l, r); // new pivot position
    qSort(a, l, p-1);
    qSort(a, p+1, r);
}

int partition(int[] a, int l, int r) {
    int p = a[l]; // the pivot value (first element)
    swap(a, l, r); // bring pivot to safety on the far right
    int j = l; // inserting position
    for (int i = l; i < r; i++) { // loop, not including the pivot
        if (a[i] < p) {
            swap(a, i, j); // sorting: shift element to the left
            j++; // shift inserting position one step to the right
        }
    }
    swap(a, j, r); // place pivot correctly
    return j;
}

int choosePivot(int l, int r) {
    return rand.nextInt(r-l) + l;
}
\end{javacode}

\subsection{Comparing QuickSort with MergeSort}

Comparing \emph{QickSort} with \emph{MergeSort}, we see that they work quite similiar, with one subtle difference:
in \emph{MergeSort} the main method first calls itself until the input is split into $n$ subsets, and then the worker method merges together the subsets in recursive order.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{../img/sortAnimation.png}
\caption[Sorting in Action]{Snapshot of a sorting animation}
\end{figure}

\emph{QickSort} however, at first calls the worker method and then itself on the resulting subsets.
This leads to two observations:
\begin{itemize}
    \item the chosen pivot value is placed at the correct final position in every turn and is never touched again; therefor the input structure can be used for \emph{in-place-updates} of the resulting subsets for every next turn;
    \item the number of steps in the inner loop (worker method \emph{partition}) is less than the number of steps in the inner loop of \emph{MergeSort} (worker method \emph{merge}), where the elements have to be copied into an auxiliary structure and from there back to the original structure.
\end{itemize}

For these reasons, even if we consider the additional steps of choosing a random value and a small constant factor for not always finding a good pivot, there is still a good chance that \emph{QickSort} will outperform \emph{MergeSort} in an imperative setting.

As we have seen with implementation \ref{impl:qshaskell} (\nameref{impl:qshaskell}), these assumptions do not hold in a functional setting: neither is it possible to perform \emph{in-place-updates} nor can we access an arbitrary element at constant time.
Here, \emph{MergeSort} is likely to be the better alternative. 

\subsection{Excursion: Greedy Sorting Algorithms with Haskell}

For small to midsize problems in a functional setting, we can always fall back to \emph{InsertionSort} and \emph{SelectionSort}, which are stable and smooth and fairly easy to implement.
They are examples of \textbf{greedy algorithms}, which we will explore in detail in a later chapter.

For now, let us just characterize greedy algorithms as a fusion of two functions: one that selects a \emph{best candidate} from another function that generates \emph{all candidates}, or at least all candidates that may turn out to be best ones.

For sorting, of course, it would not be suitable to generate all possible permutations of a given list ahead of time; instead, we will compare everey element in the list to every other element in order to find its correct position for the final output.
Thus, we have to run through the list twice in nested loops, leading to an overall running time of $\Theta (n^2)$.

This is a nice example for \emph{greedy algorithms} in general: instead of generating all possible permutations of the search space, we will exploit certain \emph{monotonicity conditions} about it and then use these to reduce the search space.
For our current problem of sorting this means that we only generate pairs of elements with the first one being smaller or equal to the second one; thus we are using the monotonous ascending order of an already sorted list.

\begin{impl}[InsertionSort with Haskell]
\end{impl}

\begin{haskellcode}
iSort :: Ord a => [a] -> [a]
iSort = foldr insert []

insert :: Ord a => a -> [a] -> [a]
insert x [] = [x]
insert x (y:ys) | x <= y = x:y:ys
                | otherwise = y : insert x ys
\end{haskellcode}

\begin{impl}[SelectionSort with Java]
\end{impl}

\begin{javacode}
void sSort(int[] a) {
  for (int i = 0; i < n; i++) {
    for (int j = i + 1; j < n; j++) {
      if (a[j] < a[i]) swap(a, i, j);
    }
  }

void swap(int[] a, int i, int j) {
  int t = a[i];
  a[i] = a[j];
  a[j] = t;
}
\end{javacode}

\subsection{Benchmarking the Sorting Algorithms}

Now let's have a look at the grapes of our labour.
In the following benchmark report we have two groups, the first (the yellow one) shows the results of sorting a small list of 1,000 random integers, the second (blue) one shows the same for a large list with 1,000,000 random integers. All measures are taken from the Haskell versions.

\emph{QuickSort} is a little faster than \emph{Smooth MergeSort} on the small set, wherase \emph{MergeSort} catches up on the large set.
Notice that the built-in sorting function \texttt{Data.List.sort} is implemented as a smooth merge sort as well, but it is even more sophisticated in that it splits the input into both ascending runs and and descending runs, taking care to reverse the descending runs.
It is more than twice as fast as our implementations (observe the logarithmic scale on the time axis). 

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{../img/bench_sort.png}
\caption[Sorting benchmarks]{Benchmarking our sorting algorithms}
\end{figure}

\section{HeapSort}

In this section we are going to use a data structure called \emph{heap} for sorting. We will start with a functional perspective, and then switch to an imperative perspective to show the fundamental differences between both.

\subsection{Functional HeapSort}

\begin{defn}[Heap]
As \textbf{heap} is a size-balanced binary tree in which the value at a node is no larger than the values in eihter of its subtrees.
\end{defn}

Thus, flattening a heap produces a list in nondeceasing (not strictly ascending) order.
The idea is now to create a heap-ordered tree from a list and then flatten the tree in order to obtain a sorted list.

\begin{impl}[HeapSort with Haskell]
In this version of \texttt{HeapSort} we are defining a main function \texttt{mkheap}, which calls itself on a splitted input.
The worker function \texttt{split} splits the input into three components: a single Node (wich is the smallest element of the list) and two sublists of equal length.
\texttt{split} is inherently reccursive, but uses the function \texttt{foldr} to iterate over the input at once.
\end{impl}

\begin{haskellcode}
data Tree a = Null | Node a (Tree a) (Tree a)

mkheap :: Ord a => [a] -> Tree a
mkheap [] = Null
mkheap (x:xs) = Node y (mkheap ys) (mkheap zs)
    where (y, ys, zs) = split(x:xs)

split :: Ord a => [a] -> (a, [a], [a])
split (x:xs) = foldr op (x, [], []) xs
    where op x (y, ys, zs) | x <= y    = (x, y:zs, ys)
                           | otherwise = (y, x:zs, ys)

flatten :: Ord a => Tree a -> [a]
flatten Null = []
flatten (Node x u v) = x:merge (flatten u) (flatten v)

hSort :: Ord a => [a] -> [a]
hSort = flatten . mkheap
\end{haskellcode}

The function \texttt(flatten) is comparable to our basic \emph{MergeSort} implementation; it has a running time of $\Theta(n \lg n)$.
But \texttt{mkheap} has also a \emph{linearithmic} running time, so joining these two fuctions for \texttt{heapsort} is certainly slower than \emph{MergeSort}.
However, it is possible to build a \emph{heap} in linear time by building a size-balanced tree with $\Theta(n)$ steps at first and then \emph{heapify} that tree with $\Theta(n)$ steps as well.
We will not proceed on that idea here, because even then \texttt{hSort} will not be as fast as \emph{MergeSort} in a functional setting.
On the other hand, heaps are useful for other purposes like the implemenation of \emph{Priority Queues}, a topic we will take up in a later chapter.

\subsection{Imperative HeapSort}

The reason why we have split the section on \emph{HeapSort} into two separate subsections is simply that \emph{heaps} in an imperative setting are represented fundamentally different in respect to a functional setting.
Sarting with the definition\footnote{as taken from \autocite[section 9.2][]{algsc98}} of a \emph{heap}, we see that heaps do not have to be stored as trees:

\begin{defn}[Heap]
A \textbf{heap} is a set of nodes with keys arranged in a complete heap-ordered binary tree, represented as an array.
\end{defn}

\begin{defn}[Heap-ordering]
A tree is \textbf{heap-ordered} if the key in each node is larger than or equal to the keys in all of that node's children.
\end{defn}

Note, that this ordering is opposite to the ordering in a functional setting.
We could also use a linked representation for heap-ordered structures, but complete trees provide us with the opportunity to use a compact arrary representation where we can easily get from a note to its parent and children without needing to maintain links.
The parent of the node in position $i$ is in position $\lfloor i/2 \rfloor$ and, conversely, its children are in position $2i$ and $2i+1$.
This arrangement makes traversal of such a structure even easier than if the data were implemented with an actual tree.
Additionally, this representation gives us access to every element in constant time, wherase we would need $\Theta(\lg n)$ steps for that in a tree structure.

\begin{impl}[HeapSort with Java]
The main function \texttt{hSort} first constructs the heap within its for-loop; the while-loop exchanges the largest element with the final element in the array and restores the heap, continuing until the heap is empty.
\texttt{heapify} moves down the heap, exchanging the node at position k with the larger of that node's two children if necessary and stopping when the node at k is not smaller than either child or the bottom is reached.
This impementation is fast with $\Theta(n \lg n)$ steps and compact, since we are using in-place-updates, restoring the heap-ordering within the input array. But it's not stable nor smooth.
\end{impl}

\begin{javacode}
public void hSort(int[] a, int r) {
    for (int k = r/2; k >= 0; k--)
        heapify(a, k, r);
    while (r > 0) {
        swap(a, 0, r);
        heapify(a, 0, --r);
    }
}

void heapify(int[] a, int k, int r) {
    int j;
    while (2*k <= r) {
        j = 2*k;
        if (j < r && a[j] < a[j+1]) j++;
        if (a[k] >= a[j]) break;
        swap(a, k, j);
        k = j;
    }
}
\end{javacode}

\section{Running the Haskell Code}

\begin{haskellcode*}{gobble=2}

> import qualified Sorting as S
> import qualified Random as R
> 
> main :: IO ()
> main = do
>   let ints = R.randInt 10 42 (0, 100)
>   print $ S.mSort ints
>   print $ S.bumSort ints
>   print $ S.smoothMS ints
>   print $ S.qSort ints
>   print $ S.iSort ints
>   print $ S.sSort ints
>   print $ S.hSort ints

\end{haskellcode*}
