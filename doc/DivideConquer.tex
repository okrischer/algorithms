\chapter{Divide-and-Conquer}

The fundamental idea of \emph{Divide-and-Conquer} algorithms is to divide a given problem into one or more subproblems, each of them beeing smaller than the original problem, to solve each of these subproblems, and then combine the solutions to give a solution to the original problem.

Each subproblem is simply the original problem on an input of smaller size; hence all subproblems are solved by the same strategy. A \emph{Divide-and-Conquer} algorithm is therefore essentially recursive in nature.

The subproblems are independent of each other (i.e. they do not share the same input data); hence they can be solved concurrently as well as sequentially. If the subproblems are not independent of each other, they could be tackled by the \emph{Dynamic Programming} strategy, which we will cover later in this text.

\emph{Divide-and-Conquer} algoritms are typically implemented within two separate procedures:
\begin{itemize}
\item \textbf{master}: the callable procedure, dividing up the work recursively and collecting the results from the worker method;
\item \textbf{worker}: repeatedly invoked procedure, computing the result for a subset of the original problem.
\end{itemize}

\emph{Divide-and-Conquer} algorithms are often used for \emph{sorting} and \emph{searching}, for which we will discover some examples in this chapter.
Before going into details of actual implementations, let us first define some desired qualities of sorting algorithms and algorithms in general. Algorithms should be
\begin{itemize}
\item \textbf{fast}: this does not only concern the asymptotic runtime, which is relevant for big input sizes, but also means reducing the number of steps for each iteration of the algorithm; 
\item \textbf{smooth}: the more sorted the input is, the faster the algorithm should perform; in general, if the problem at hand is already solved for the majority of inputs values, the algorithm should be faster;
\item \textbf{stable}: sorting should not affect the presorted order of equal elements, i.e. the order of elements of a complex datatype with the same key should not be altered;
\item \textbf{compact}: the algorithm should be economical in its use of space as well as running time
\end{itemize}

We will see that a combination of these qualities for a distinct algorithm is often not easy to accomplish; especially the \emph{compactness} of an algorithm is diffucult to achieve in a prurely functional setting.

\section{Binary Search}

\section{MergeSort}

\emph{MergeSort} is a stable and fast sorting algorithm with a guaranteed runtime of $\Theta (n \lg n)$.
Compared to linear sorting algorithms like \emph{SelectionSort} and \emph{InsertionSort}, which share runtime of $\Theta (n^2)$, this means a huge performance advantage, as $\lg n$ grows much slower than $n$ itself for increasing input sizes.  
The downside of \emph{MergeSort} is the need of extra space (of size $n$), as no \emph{in-place-update} of the input structure is possible. So it is not compact. Likewise, \emph{MergeSort} isn't smooth, meaning that the algorithm will always take $\Theta (n \lg n)$ steps, even if the input is already sorted.

\begin{impl}[MergeSort with Haskell] \label{impl:mshaskell}
This implementation uses recursion for the master and worker method, as this is the most natural (and in a pure functional language the only) way to iterate through a data structure.
Observe, that there is no need of creating an explicit data structure for merging, as data structures are always build from scratch in a functional language with immutable data.
Also note the helper function \texttt{halve}, which divides an input list into to halves without traversing the input multiple times.
\end{impl}

\begin{haskellcode}
  mSort :: Ord a => [a] -> [a]
  mSort [] = []
  mSort [x] = [x]
  mSort xs = merge (mSort left) (mSort right)
             where (left, right) = halve xs

  halve :: [a] -> ([a], [a])
  halve = foldr op ([], []) where op x (ys, zs) = (zs, x:ys)

  merge :: Ord a => [a] -> [a] -> [a]
  merge [] ys = ys
  merge xs [] = xs
  merge left@(x:xs) right@(y:ys)
      | x <= y    = x : merge xs right
      | otherwise = y : merge left ys
\end{haskellcode}

\begin{impl}[MergeSort with Java] 
This implementation uses an auxiliary array for merging, which is filled in bitonic order:
the two subarrays for each recursion step are copied `back-to-back' into this array, such that the left subarray has ascending order, the right one descending order\footnote{as suggested in \autocite[chapter 8]{algsc98}}.
The actual sorting is done by comparing the first with the last element and copying the smaller one back into the original data structure. This implementation is fast and stable.
\end{impl}

\begin{javacode}
void mSort(int l, int r) {
    int m = (r+l) / 2; // dividing input in two halves
    if (l >= r) return; // already sorted
    mSort(l, m);
    mSort(m+1, r);
    merge(l, m, r); // sort by merging
}

void merge(int l, int m, int r) {
    int i, j;
    // copy the left half to aux
    for (i = m+1; i > l; i--) aux[i-1] = a[i-1];
    // copy the right half in reverse order to aux
    for (j = m; j < r; j++) aux[r+m-j] = a[j+1];
    // compare first and last value of aux and copy smaller to a
    for (int k = l; k <= r; k++) {
        if (aux[i] < aux[j]) a[k] = aux[i++];
        else a[k] = aux[j--];
    }
}
\end{javacode}

Observe that in both implementations the intermediate data structures are exclusively accessed in a sequential style, i.e. there is no need of random acces to an element at any stage of the process.
This, and the fact that the mandatory extra space is created anyway in a pure functional language makes \emph{MergeSort} a good choise for sorting in a functional setting with immutable data.

\subsection{A Functional Excursion on MergeSort}
It is wortwhile to explore some other, more sophisticated versions of \emph{MergeSort}, which will combine two or more of the desired qualities of algorithms.

Our basic implementation \ref{impl:mshaskell} (\nameref{impl:mshaskell}) is fast but not stable, due to the definition of the helper function \texttt{halve}.
This definition makes the algorithm unstable, as the order of the input elements is altered before sorting.
We could replace this definition with {\mintinline{haskell}{halve xs = (take m xs, drop m xs) where m = length xs div 2}}, and this would make the algorithm stable but also slower at the same time, as this definition has to iterate three times over the input list.

In particular, we will explore \emph{Bottom-up} MergeSort, which switches from the divide-and-conquer scheme to a bottom-up scheme, and \emph{Smooth} MergeSort, a variant of Bottom-up with multiple runs.

\begin{impl}[Bottom-up MergeSort with Haskell]
\end{impl}

\section{QickSort}

\emph{QuickSort} is a \emph{Divide-and-Conquer} algorithm as well, althoug it is sometimes referred to as a \emph{partitioning} or \emph{randomized} algorithm.
\emph{QuickSort} is stable, fast (assuming non-sorted input) and compact; there is no need for an intermediate data structure because \emph{in-place-updates} are possible.

The main downside is that it is not smooth; on the contrary, the running will be $\Theta (n^2)$ for an already sorted input and extra care has to be taken in order to avoid this quadratic running time.

This behaviour is due to the choice of the pivot value, which is used to split the input list into two sublists, one with values smaller than the pivot, the other with values greater or equal than the pivot.

If the input is already sorted and we take the pivot always from the first position of the input, the list is split into an empty sublist and another with length $n-1$ in every turn, accumulating to $n$ partitioning steps for the whole algorithm.
The average running time is $\Theta (n \lg n)$, assuming non-sorted input.

\begin{impl}[QuickSort with Haskell] \label{impl:qshaskell}
Here are two implementations: the first one shows the simplicity of \texttt{QuickSort} in a functional setting.
The second version reduces additional space from $\Theta (n^2)$ to $\Theta (n)$ by explicitly partitioning the input into two sublists.
Observe that no care is taken in either case to choose a good pivot value, as this would lead to extra complexity in a functional setting, not improving the average runtime. As a result, this implementation is not fast, not smooth and not compact, but stable.
\end{impl}

\begin{haskellcode}
  qSort :: Ord a => [a] -> [a]
  qSort [] = []
  qSort (x:xs) = qSort [y | y <- xs, y < x] ++ [x] ++
                 qSort [y | y <- xs, y >= x]

  qSort :: Ord a => [a] -> [a]
  qSort [] = []
  qSort (x:xs) = qSort ys ++ [x] ++ qSort zs
      where (ys, zs) = partition (<x) xs

  partition :: (a -> Bool) -> [a] -> ([a], [a])
  partition p = foldr op ([], [])
      where op x (ys, zs) = if p x then (x:ys, zs) else (ys, x:zs)
\end{haskellcode}

\begin{impl}[QuickSort with Java]
In order to avoid a quadratic runtime for already sorted inputs, the master method \texttt{qSort} calls a helper method \texttt{choosePivot} to randomly pick a value from within the given range, and puts this value in the first position.
The sorting is done in the worker method \texttt{partition}, where the pivot value is always taken from the first element (the randomly choosen element).
In each partitioning step the current pivot value is copied into its final position by shifting every smaller value to the left. With the random pivot value this implementation is fast, stable and compact, but not smooth.
\end{impl}

\begin{javacode}
void qSort(int l, int r) {
    if (l >= r) return; // already sorted
    int i = choosePivot(l, r);
    swap(l, i); // make chosen pivot the first element
    int p = partition(l, r); // new pivot position
    qSort(l, p-1);
    qSort(p+1, r);
}

int partition(int l, int r) {
    int p = a[l]; // the pivot value (first element)
    swap(l, r); // bring pivot to safety on the far right
    int j = l; // inserting position
    for (int i = l; i < r; i++) { // loop, not including the pivot
        if (a[i] < p) {
            swap(i, j); // sorting: shift element to the left
            j++; // shift inserting position one step to the right
        }
    }
    swap(j, r); // place pivot correctly
    return j;
}

int choosePivot(int l, int r) {
    Random rand = new Random();
    return rand.nextInt(r-l) + l;
}
\end{javacode}

\subsection{Comparing QuickSort with MergeSort}
Comparing \emph{QickSort} with \emph{MergeSort}, we see that they work quite similiar, with one subtle difference:
in \emph{MergeSort} the master method first calls itself until the input is split into $n$ subsets, and then the worker method merges together the subsets in recursive order.

\emph{QickSort} however, at first calls the worker method and then itself on the resulting subsets.
This leads to two observations:
\begin{itemize}
    \item the chosen pivot value is placed at the correct final position in every turn and is never touched again; therefor the input structure can be used for \emph{in-place-updates} of the resulting subsets for every next turn;
    \item the number of steps in the inner loop (worker method \emph{partition}) is less than the number of steps in the inner loop of \emph{MergeSort} (worker method \emph{merge}), where the elements have to be copied into an auxiliary structure and from there back to the original structure.
\end{itemize}

For these reasons, even if we consider the additional steps of choosing a random value and a small constant factor for not always finding a good pivot, there is still a good chance that \emph{QickSort} will outperform \emph{MergeSort} in an imperative setting.

As we have seen with implementation \ref{impl:qshaskell} (\nameref{impl:qshaskell}), these assumptions do not hold in a functional setting: neither is it possible to perform \emph{in-place-updates} nor can we access an arbitrary element at constant time.
Here, \emph{MergeSort} is likely to be the better alternative. 
And for small to midsize problems we can always fall back to \emph{InsertionSort}, which is stable, smooth and fast on linked lists (although its running time is still $\Theta (n^2)$, which matters for big inputs).

\begin{impl}[InsertionSort with Haskell]
\end{impl}

\begin{haskellcode}
  iSort :: Ord a => [a] -> [a]
  iSort = foldr insert []

  insert :: Ord a => a -> [a] -> [a]
  insert x [] = [x]
  insert x (y:ys) | x <= y = x:y:ys
                  | otherwise = y : insert x ys
\end{haskellcode}

\section{Counting Inversions}

\section{Strassen's Matrix Multiplication Algorithm}

\section{Closest Pair}

\section{Running the Haskell Code}

\begin{haskellcode}

> import qualified DivConq as DC
> import qualified Random as R
> 
> main :: IO ()
> main = do
>   let ints = R.randInt 10 42 (0, 100)
>   print $ DC.mSort ints
>   print $ DC.qSort ints
>   print $ DC.iSort ints

\end{haskellcode}
