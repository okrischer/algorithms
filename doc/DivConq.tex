\section{Divide-and-Conquer}


\begin{haskellcode}

> import qualified DivConq as DC
> import qualified Random as R

\end{haskellcode}


\subsection{Merge Sort}

\emph{\texttt{MergeSort}} is the canonical example for \emph{Divide-and-Conquer} algorithms.
These algorithms are typically implemented within two methods:
\begin{itemize}
    \item \texttt{master}: the callable method, dividing up the work recursively and collecting the results from the worker method;
    \item \texttt{worker}: repeatedly invoked helper method, computing the result for a subset of the original problem.
\end{itemize}

The main feature of \emph{\texttt{MergeSort}} is that it is a \emph{stable} linear sorting algorithm with a guaranteed runtime complexity of $\Theta (n \lg n)$.
An algorithm is considered as \emph{stable} if the runtime doesn't change due to the form of input data (ordered, unordered or partially ordered).
The downside of this appoach is the need of extra space (of size $n$), as no \emph{in-place-update} of the input structure is possible.

\begin{impl}[MergeSort with Haskell]
This implementation uses recursion for both methods, as this is the most natural (and in a pure functional language the only) way to iterate through a data structure.
Observe, that there is no need of creating an explicit data structure for merging, as data structures are always build from scratch in a functional language with immutable data.
\end{impl}

\begin{haskellcode}
  mSort :: Ord a => [a] -> [a]
  mSort [] = []
  mSort [x] = [x]
  mSort xs = merge (mSort left) (mSort right)
      where
      (left, right) = splitAt (length xs `div` 2) xs

  merge :: Ord a => [a] -> [a] -> [a]
  merge [] ys = ys
  merge xs [] = xs
  merge left@(x:xs) right@(y:ys)
      | x <= y    = x : merge xs right
      | otherwise = y : merge left ys
\end{haskellcode}

\begin{impl}[MergeSort with Java]
This implementation uses an auxiliary array for merging, which is filled in bitonic order:
the two subarrays for each recursion step are copied `back-to-back' into this array, such that the left subarray has ascending order, the right one descending.
The actual sorting is done by comparing the first with the last element and copying the smaller one back into the original data structure.
\end{impl}

\begin{javacode}
void mSort(int l, int r) {
    int m = (r+l) / 2; // dividing input in two halves
    if (l >= r) return; // already sorted
    mSort(l, m);
    mSort(m+1, r);
    merge(l, m, r); // sort by merging
}

void merge(int l, int m, int r) {
    int i, j;
    // copy the left half to aux
    for (i = m+1; i > l; i--) aux[i-1] = a[i-1];
    // copy the right half in reverse order to aux
    for (j = m; j < r; j++) aux[r+m-j] = a[j+1];
    // compare first and last value of aux and copy smaller to a
    for (int k = l; k <= r; k++) {
        if (aux[i] < aux[j]) a[k] = aux[i++];
        else a[k] = aux[j--];
    }
}
\end{javacode}

Observe that in both implementations the intermediate data structures are exclusively accessed in a sequential style, i.e. there is no need of random acces to an element at any stage of the process.
This, and the fact that the mandatory extra space is created anyway in a pure functional language makes \emph{\texttt{MergeSort}} a good choise for sorting in a functional setting with immutable data.

\subsection{Qick Sort}

\emph{\texttt{QuickSort}} is a \emph{Divide-and-Conquer} algorithm as well, althoug it is sometimes referred to as a \emph{partitioning} or \emph{randomized} algorithm.
The main advantage of \emph{\texttt{QuickSort}} is its simplicity and the omission of an intermediate data structure, as \emph{in-place-update} is possible.
The main downside is that it is not stable and extra care has to be taken in order to avoid a quadratic running time for already sorted inputs. Its average running time is then $\Theta (n \lg n)$.

\begin{impl}[QuickSort with Haskell] \label{impl:qshaskell}
We are giving two implementations: the first one to show the simplicity of \texttt{QuickSort}; the second one reduces space complexity from $\Theta (n^2)$ to $\Theta (n)$ by explicitly partitioning the input into two sublists.
Observe that no care is taken in either case to choose a good pivot value, as this would lead to extra complexity in a functional setting, not improving the average runtime.
\end{impl}

\begin{haskellcode}
  qSort :: Ord a => [a] -> [a]
  qSort [] = []
  qSort (x:xs) = qSort [y | y <- xs, y < x] ++ [x] ++
                 qSort [y | y <- xs, y >= x]

  qSort :: Ord a => [a] -> [a]
  qSort [] = []
  qSort (x:xs) = pSort xs [] []
      where
      pSort [] us vs = qSort us ++ [x] ++ qSort vs
      pSort (y:xs) us vs = if y < x
                           then pSort xs (y:us) vs
                           else pSort xs us (y:vs)
\end{haskellcode}

\begin{impl}[QuickSort with Java]
In order to avoid a quadratic runtime for already sorted inputs, the master method \texttt{qSort} calls a helper method \texttt{choosePivot} to randomly pick a value from within the given range, and puts this value in the first position.
The sorting is done in the worker method \texttt{partition}, where the pivot value is always taken from the first element (the randomly choosen element).
In each partitioning step the current pivot value is copied into its final position by shifting every smaller value to the left, and it is never touched again.
\end{impl}

\begin{javacode}
void qSort(int l, int r) {
    if (l >= r) return; // already sorted
    int i = choosePivot(l, r);
    swap(l, i); // make chosen pivot the first element
    int p = partition(l, r); // new pivot position
    qSort(l, p-1);
    qSort(p+1, r);
}

int partition(int l, int r) {
    int p = a[l]; // the pivot value (first element)
    swap(l, r); // bring pivot to safety on the far right
    int j = l; // inserting position
    for (int i = l; i < r; i++) { // loop, not including the pivot
        if (a[i] < p) {
            swap(i, j); // sorting: shift element to the left
            j++; // shift inserting position one step to the right
        }
    }
    swap(j, r); // place pivot correctly
    return j;
}

int choosePivot(int l, int r) {
    Random rand = new Random();
    return rand.nextInt(r-l) + l;
}
\end{javacode}

Comparing \emph{\texttt{QickSort}} with \emph{\texttt{MergeSort}}, we see that they work quite similiar, with one subtle difference:
in \emph{\texttt{MergeSort}} the master method first calls itself until the input is split into $n$ subsets, and then the worker method merges together the subsets in recursive order.

\emph{\texttt{QickSort}} first calls the worker method and then itself on the resulting subsets.
This leads to two observations:
\begin{itemize}
    \item the chosen pivot value is placed at the correct final position in every turn and is never touched again; therefor the input structure can be used for \emph{in-place-updates} of the resulting subsets for every next turn.
    \item the number of steps in the inner loop (worker method \emph{partition}) is less than the number of steps in the inner loop of \emph{\texttt{MergeSort}} (worker method \emph{merge}), where the elements have to be copied into an auxiliary structure and from there back to the original structure.
\end{itemize}

For these reasons, even if we consider the additional steps of choosing a random value and a small constant factor for not always finding a good pivot, there is still a good chance that \emph{\texttt{QickSort}} will outperform \emph{\texttt{MergeSort}} in an imperative setting.

As we have seen with the the Haskell implementation \ref{impl:qshaskell}, these assumptions do not hold in a functional setting, where we neither can perform \emph{in-place-updates}, nor do we have access to an arbitrary element at constant time. Here, \emph{\texttt{MergeSort}} is likely to be the better alternative; and for small to midsize problems, we can always fall back to \emph{\texttt{InsertionSort}}, which is very fast on linked lists.

\begin{impl}[InsertionSort with Haskell]
\end{impl}

\begin{haskellcode}
  iSort :: Ord a => [a] -> [a]
  iSort = foldr insert []

  insert :: Ord a => a -> [a] -> [a]
  insert x [] = [x]
  insert x (y:ys) | x <= y = x:y:ys
                  | otherwise = y : insert x ys
\end{haskellcode}

\subsection{Counting Inversions}

\subsection{Strassen's Matrix Multiplication Algorithm}

\subsection{Closest Pair}

\subsection{Running the Haskell Code}

\begin{haskellcode}

> main :: IO ()
> main = do
>   let ints = R.randInt 10 42 (0, 100)
>   print $ DC.mSort ints
>   print $ DC.qSort ints
>   print $ DC.iSort ints

\end{haskellcode}
